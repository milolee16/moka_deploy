import os
import json
import joblib
import threading
import uuid
import time
from datetime import datetime, timedelta
from collections import Counter
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from flask import Flask, request, jsonify
from flask_cors import CORS

app = Flask(__name__)
CORS(app, resources={
    r"/*": {
        "origins": ["http://localhost:3000", "http://127.0.0.1:3000"],
        "methods": ["GET", "POST", "OPTIONS"],
        "allow_headers": ["Content-Type"]
    }
})

class LocalMLManager:
    def __init__(self):
        # ìŠ¤ë ˆë“œ ì•ˆì „ì„±ì„ ìœ„í•œ ë½ ì¶”ê°€
        self.model_lock = threading.RLock()  # ì¬ì§„ì… ê°€ëŠ¥í•œ ë½
        self.data_lock = threading.RLock()   # ë°ì´í„° ì ‘ê·¼ìš© ë½
        
        self.model_path = "models"
        self.training_data = []
        self.prediction_log = []
        self.feedback_data = []
        self.intent_classifier = None
        self.vectorizer = None
        
        # ë””ë ‰í† ë¦¬ ìƒì„±
        if not os.path.exists(self.model_path):
            os.makedirs(self.model_path)
        
        # ì €ì¥ëœ ëª¨ë¸ ë¡œë“œ ì‹œë„
        self.load_models()
        
        print(f"[AutoML] LocalMLManager ì´ˆê¸°í™” ì™„ë£Œ (ìŠ¤ë ˆë“œ ì•ˆì „)")
    
    def add_training_data(self, text, intent, confidence=0.5):
        """í›ˆë ¨ ë°ì´í„° ì¶”ê°€ - ìŠ¤ë ˆë“œ ì•ˆì „"""
        with self.data_lock:
            self.training_data.append({
                'text': text,
                'intent': intent,
                'confidence': confidence,
                'timestamp': datetime.now()
            })
            print(f"[AutoML] í›ˆë ¨ ë°ì´í„° ì¶”ê°€: {text[:30]}... -> {intent}")
    
    def train_intent_classifier(self):
        """ì˜ë„ ë¶„ë¥˜ ëª¨ë¸ í›ˆë ¨ - ìŠ¤ë ˆë“œ ì•ˆì „"""
        with self.model_lock:  # ëª¨ë¸ í›ˆë ¨ ì¤‘ ë‹¤ë¥¸ ì‘ì—… ì°¨ë‹¨
            print("[AutoML] ëª¨ë¸ í›ˆë ¨ ì‹œì‘...")
            
            # ë°ì´í„° ì ‘ê·¼ ì‹œì—ë„ ë½ ì‚¬ìš©
            with self.data_lock:
                if len(self.training_data) < 10:
                    print(f"[AutoML] í›ˆë ¨ ë°ì´í„° ë¶€ì¡±: {len(self.training_data)}ê°œ (ìµœì†Œ 10ê°œ í•„ìš”)")
                    return False
                
                # ë°ì´í„° ë³µì‚¬ (ë½ ë²”ìœ„ ìµœì†Œí™”)
                training_data_copy = self.training_data.copy()
            
            # ë°ì´í„° ì¤€ë¹„
            texts = [item['text'] for item in training_data_copy]
            intents = [item['intent'] for item in training_data_copy]
            
            # í´ë˜ìŠ¤ë³„ ë°ì´í„° ìˆ˜ í™•ì¸
            intent_counts = Counter(intents)
            min_samples_per_class = min(intent_counts.values())
            unique_classes = len(intent_counts)
            
            print(f"[AutoML] í´ë˜ìŠ¤ë³„ ë°ì´í„° ìˆ˜: {dict(intent_counts)}")
            print(f"[AutoML] ì´ ë°ì´í„°: {len(texts)}ê°œ, í´ë˜ìŠ¤: {unique_classes}ê°œ")
            
            try:
                # í…ìŠ¤íŠ¸ ë²¡í„°í™”
                self.vectorizer = TfidfVectorizer(
                    max_features=1000,
                    ngram_range=(1, 2),
                    stop_words=None
                )
                X = self.vectorizer.fit_transform(texts)
                
                # ëª¨ë¸ í›ˆë ¨
                self.intent_classifier = RandomForestClassifier(
                    n_estimators=100,
                    random_state=42,
                    class_weight='balanced'
                )
                
                # ë°ì´í„° ë¶„í•  ì¡°ê±´ í™•ì¸ ë° ì¡°ì •
                if len(texts) >= 20 and unique_classes > 1 and min_samples_per_class >= 2:
                    try:
                        X_train, X_test, y_train, y_test = train_test_split(
                            X, intents, test_size=0.2, random_state=42, stratify=intents
                        )
                        self.intent_classifier.fit(X_train, y_train)
                        
                        # ì„±ëŠ¥ í‰ê°€
                        y_pred = self.intent_classifier.predict(X_test)
                        accuracy = accuracy_score(y_test, y_pred)
                        print(f"[AutoML] ì˜ë„ ë¶„ë¥˜ ì •í™•ë„ (stratified): {accuracy:.3f}")
                        
                    except ValueError as e:
                        print(f"[AutoML] Stratify ì‹¤íŒ¨, ì¼ë°˜ ë¶„í• ë¡œ ì§„í–‰: {e}")
                        X_train, X_test, y_train, y_test = train_test_split(
                            X, intents, test_size=0.2, random_state=42
                        )
                        self.intent_classifier.fit(X_train, y_train)
                        
                        y_pred = self.intent_classifier.predict(X_test)
                        accuracy = accuracy_score(y_test, y_pred)
                        print(f"[AutoML] ì˜ë„ ë¶„ë¥˜ ì •í™•ë„ (non-stratified): {accuracy:.3f}")
                        
                else:
                    print(f"[AutoML] ë°ì´í„° ë¶€ì¡±ìœ¼ë¡œ ì „ì²´ ë°ì´í„°ë¡œ í›ˆë ¨ (ê²€ì¦ ì—†ìŒ)")
                    self.intent_classifier.fit(X, intents)
                
                # ëª¨ë¸ ì €ì¥
                self.save_models()
                print("[AutoML] ëª¨ë¸ í›ˆë ¨ ì™„ë£Œ")
                return True
                
            except Exception as e:
                print(f"[AutoML] ëª¨ë¸ í›ˆë ¨ ì˜¤ë¥˜: {e}")
                return False
    
    def predict_intent(self, text):
        """ì˜ë„ ì˜ˆì¸¡ - ìŠ¤ë ˆë“œ ì•ˆì „"""
        # ëª¨ë¸ ì ‘ê·¼ ì‹œ ì½ê¸° ë½ (ë‹¤ë¥¸ ì˜ˆì¸¡ê³¼ ë™ì‹œ ì‹¤í–‰ ê°€ëŠ¥)
        with self.model_lock:
            if not self.intent_classifier or not self.vectorizer:
                return None, 0.0
            
            try:
                X = self.vectorizer.transform([text])
                intent = self.intent_classifier.predict(X)[0]
                confidence = max(self.intent_classifier.predict_proba(X)[0])
                
                # ì˜ˆì¸¡ ë¡œê·¸ ì €ì¥ (ë°ì´í„° ë½ ì‚¬ìš©)
                with self.data_lock:
                    self.prediction_log.append({
                        'text': text,
                        'predicted_intent': intent,
                        'confidence': confidence,
                        'timestamp': datetime.now()
                    })
                
                return intent, confidence
                
            except Exception as e:
                print(f"[AutoML] ì˜ˆì¸¡ ì˜¤ë¥˜: {e}")
                return None, 0.0
    
    def add_feedback(self, text, predicted_intent, actual_intent, user_satisfied=True):
        """ì‚¬ìš©ì í”¼ë“œë°± ìˆ˜ì§‘ - ìŠ¤ë ˆë“œ ì•ˆì „"""
        with self.data_lock:
            self.feedback_data.append({
                'text': text,
                'predicted_intent': predicted_intent,
                'actual_intent': actual_intent,
                'user_satisfied': user_satisfied,
                'timestamp': datetime.now()
            })
            
            # ì˜ëª»ëœ ì˜ˆì¸¡ì— ëŒ€í•´ í›ˆë ¨ ë°ì´í„° ì¶”ê°€
            if actual_intent and predicted_intent != actual_intent:
                self.add_training_data(text, actual_intent, confidence=1.0)
    
    def auto_retrain(self):
        """ìë™ ì¬í›ˆë ¨ - ìŠ¤ë ˆë“œ ì•ˆì „"""
        with self.data_lock:
            feedback_count = len(self.feedback_data)
        
        if feedback_count >= 20:
            print("[AutoML] ìë™ ì¬í›ˆë ¨ ì‹œì‘...")
            success = self.train_intent_classifier()
            if success:
                with self.data_lock:
                    self.feedback_data = []
                print("[AutoML] ìë™ ì¬í›ˆë ¨ ì™„ë£Œ")
    
    def save_models(self):
        """ëª¨ë¸ ì €ì¥ - ìŠ¤ë ˆë“œ ì•ˆì „"""
        try:
            # ëª¨ë¸ ì €ì¥ ì‹œ ë½ ì‚¬ìš©
            with self.model_lock:
                if self.intent_classifier:
                    joblib.dump(self.intent_classifier, f"{self.model_path}/intent_classifier.pkl")
                if self.vectorizer:
                    joblib.dump(self.vectorizer, f"{self.model_path}/vectorizer.pkl")
            
            # ë°ì´í„° ì €ì¥ ì‹œ ë°ì´í„° ë½ ì‚¬ìš©
            with self.data_lock:
                training_data_copy = [
                    {**item, 'timestamp': item['timestamp'].isoformat() if isinstance(item['timestamp'], datetime) else item['timestamp']}
                    for item in self.training_data
                ]
                
                with open(f"{self.model_path}/training_data.json", 'w', encoding='utf-8') as f:
                    json.dump(training_data_copy, f, ensure_ascii=False, indent=2)
                    
            print("[AutoML] ëª¨ë¸ ì €ì¥ ì™„ë£Œ")
        except Exception as e:
            print(f"[AutoML] ëª¨ë¸ ì €ì¥ ì˜¤ë¥˜: {e}")
    
    def load_models(self):
        """ì €ì¥ëœ ëª¨ë¸ ë¡œë“œ - ìŠ¤ë ˆë“œ ì•ˆì „"""
        try:
            classifier_path = f"{self.model_path}/intent_classifier.pkl"
            vectorizer_path = f"{self.model_path}/vectorizer.pkl"
            data_path = f"{self.model_path}/training_data.json"
            
            with self.model_lock:
                if os.path.exists(classifier_path):
                    self.intent_classifier = joblib.load(classifier_path)
                    print("[AutoML] ì˜ë„ ë¶„ë¥˜ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
                
                if os.path.exists(vectorizer_path):
                    self.vectorizer = joblib.load(vectorizer_path)
                    print("[AutoML] ë²¡í„°ë¼ì´ì € ë¡œë“œ ì™„ë£Œ")
            
            with self.data_lock:
                if os.path.exists(data_path):
                    with open(data_path, 'r', encoding='utf-8') as f:
                        loaded_data = json.load(f)
                        # datetime ê°ì²´ ë³µì›
                        for item in loaded_data:
                            if isinstance(item.get('timestamp'), str):
                                try:
                                    item['timestamp'] = datetime.fromisoformat(item['timestamp'])
                                except:
                                    item['timestamp'] = datetime.now()
                        self.training_data = loaded_data
                    print(f"[AutoML] í›ˆë ¨ ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(self.training_data)}ê°œ")
                    
        except Exception as e:
            print(f"[AutoML] ëª¨ë¸ ë¡œë“œ ì˜¤ë¥˜: {e}")
    
    def get_model_stats(self):
        """ëª¨ë¸ í†µê³„ ì •ë³´ - ìŠ¤ë ˆë“œ ì•ˆì „"""
        with self.data_lock:
            training_count = len(self.training_data)
            prediction_count = len(self.prediction_log)
            feedback_count = len(self.feedback_data)
        
        with self.model_lock:
            model_loaded = self.intent_classifier is not None
            vectorizer_loaded = self.vectorizer is not None
        
        return {
            "training_data_count": training_count,
            "prediction_count": prediction_count,
            "feedback_count": feedback_count,
            "model_loaded": model_loaded,
            "vectorizer_loaded": vectorizer_loaded
        }

# --- ì„¸ì…˜ ê´€ë¦¬ (ê¸°ì¡´ê³¼ ë™ì¼í•˜ì§€ë§Œ ë½ ì‚¬ìš©) ---
conversation_sessions = {}
session_lock = threading.Lock()
MAX_MESSAGES_PER_SESSION = 20
SESSION_TIMEOUT_HOURS = 2

# ML ë§¤ë‹ˆì € ì´ˆê¸°í™”
ml_manager = LocalMLManager()

class ConversationSession:
    def __init__(self, session_id):
        self.session_id = session_id
        self.messages = []
        self.created_at = datetime.now()
        self.last_activity = datetime.now()
        self.ml_predictions = []
        # ì„¸ì…˜ë³„ ë½ ì¶”ê°€
        self.session_lock = threading.Lock()
    
    def add_message(self, role, content, ml_prediction=None):
        """ë©”ì‹œì§€ ì¶”ê°€ - ì„¸ì…˜ë³„ ìŠ¤ë ˆë“œ ì•ˆì „"""
        with self.session_lock:
            message_data = {
                "role": role,
                "content": content,
                "timestamp": datetime.now(),
                "ml_prediction": ml_prediction
            }
            self.messages.append(message_data)
            self.last_activity = datetime.now()
            
            if len(self.messages) > MAX_MESSAGES_PER_SESSION:
                self.messages = self.messages[-MAX_MESSAGES_PER_SESSION:]
    
    def get_conversation_context(self):
        """ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ ì¡°íšŒ - ìŠ¤ë ˆë“œ ì•ˆì „"""
        with self.session_lock:
            if not self.messages:
                return ""
            
            context_parts = []
            for msg in self.messages:
                role_label = "ì‚¬ìš©ì" if msg["role"] == "user" else "ì±—ë´‡"
                context_parts.append(f"{role_label}: {msg['content']}")
            
            return "\n".join(context_parts)
    
    def is_expired(self):
        """ì„¸ì…˜ ë§Œë£Œ í™•ì¸ - ìŠ¤ë ˆë“œ ì•ˆì „"""
        with self.session_lock:
            return datetime.now() - self.last_activity > timedelta(hours=SESSION_TIMEOUT_HOURS)

def get_or_create_session(session_id):
    """ì„¸ì…˜ ìƒì„± ë˜ëŠ” ì¡°íšŒ - ìŠ¤ë ˆë“œ ì•ˆì „"""
    with session_lock:
        if session_id not in conversation_sessions:
            conversation_sessions[session_id] = ConversationSession(session_id)
        return conversation_sessions[session_id]

def cleanup_expired_sessions():
    """ë§Œë£Œëœ ì„¸ì…˜ ì •ë¦¬ - ìŠ¤ë ˆë“œ ì•ˆì „"""
    with session_lock:
        expired_sessions = [
            sid for sid, session in conversation_sessions.items() 
            if session.is_expired()
        ]
        for sid in expired_sessions:
            del conversation_sessions[sid]
        
        if expired_sessions:
            print(f"[ì„¸ì…˜ ì •ë¦¬] {len(expired_sessions)}ê°œ ë§Œë£Œëœ ì„¸ì…˜ ì‚­ì œ")

def initialize_training_data():
    """ì´ˆê¸° í›ˆë ¨ ë°ì´í„° ì„¤ì •"""
    initial_data = [
        # ì˜ˆì•½ ë¬¸ì˜ (10ê°œ)
        ("ì§€ê¸ˆ ë°”ë¡œ íƒˆ ìˆ˜ ìˆëŠ” ì°¨ ìˆì–´ìš”?", "ì˜ˆì•½_ë¬¸ì˜"),
        ("ë‚´ì¼ ì•„ì¹¨ì— ì°¨ ì˜ˆì•½í•˜ê³  ì‹¶ì–´ìš”", "ì˜ˆì•½_ë¬¸ì˜"),
        ("ì°¨ëŸ‰ ì˜ˆì•½ ê°€ëŠ¥í•œê°€ìš”?", "ì˜ˆì•½_ë¬¸ì˜"),
        ("ì˜ˆì•½ ì·¨ì†Œí•˜ê³  ì‹¶ì–´ìš”", "ì˜ˆì•½_ë¬¸ì˜"),
        ("ì˜ˆì•½ ë³€ê²½ ê°€ëŠ¥í•œê°€ìš”?", "ì˜ˆì•½_ë¬¸ì˜"),
        ("ì˜¤ëŠ˜ ì €ë…ì— ì°¨ ë¹Œë¦´ ìˆ˜ ìˆë‚˜ìš”?", "ì˜ˆì•½_ë¬¸ì˜"),
        ("ì˜ˆì•½í•˜ë ¤ë©´ ì–´ë–»ê²Œ í•´ì•¼ í•˜ë‚˜ìš”?", "ì˜ˆì•½_ë¬¸ì˜"),
        ("ì°¨ ì˜ˆì•½ ì‹ ì²­í•˜ê³  ì‹¶ìŠµë‹ˆë‹¤", "ì˜ˆì•½_ë¬¸ì˜"),
        ("ë‹¹ì¼ ì˜ˆì•½ ê°€ëŠ¥í•œê°€ìš”?", "ì˜ˆì•½_ë¬¸ì˜"),
        ("ì˜ˆì•½ í™•ì¸í•˜ê³  ì‹¶ì–´ìš”", "ì˜ˆì•½_ë¬¸ì˜"),
        
        # ìš”ê¸ˆ ë¬¸ì˜ (10ê°œ)
        ("ì£¼í–‰ ìš”ê¸ˆì€ 1kmë‹¹ ì–¼ë§ˆì˜ˆìš”?", "ìš”ê¸ˆ_ë¬¸ì˜"),
        ("í•˜ë£¨ ë ŒíŠ¸ë¹„ê°€ ì–¼ë§ˆì¸ê°€ìš”?", "ìš”ê¸ˆ_ë¬¸ì˜"),
        ("ë³´í—˜ë£ŒëŠ” ë³„ë„ì¸ê°€ìš”?", "ìš”ê¸ˆ_ë¬¸ì˜"),
        ("í• ì¸ í˜œíƒ ìˆë‚˜ìš”?", "ìš”ê¸ˆ_ë¬¸ì˜"),
        ("ê²°ì œ ë°©ë²•ì´ ê¶ê¸ˆí•´ìš”", "ìš”ê¸ˆ_ë¬¸ì˜"),
        ("ìš”ê¸ˆí‘œ ë³´ì—¬ì£¼ì„¸ìš”", "ìš”ê¸ˆ_ë¬¸ì˜"),
        ("ì–¼ë§ˆë‚˜ ë¹„ì‹¸ìš”?", "ìš”ê¸ˆ_ë¬¸ì˜"),
        ("ê°€ê²©ì´ ê¶ê¸ˆí•©ë‹ˆë‹¤", "ìš”ê¸ˆ_ë¬¸ì˜"),
        ("ìš”ê¸ˆ ê³„ì‚° ë°©ë²•ì„ ì•Œë ¤ì£¼ì„¸ìš”", "ìš”ê¸ˆ_ë¬¸ì˜"),
        ("ë¹„ìš©ì´ ì–´ë–»ê²Œ ë˜ë‚˜ìš”?", "ìš”ê¸ˆ_ë¬¸ì˜"),
        
        # ì´ìš© ë°©ë²• (10ê°œ)
        ("ì°¨ ë¬¸ì€ ì–´ë–»ê²Œ ì—´ì–´ìš”?", "ì´ìš©_ë°©ë²•"),
        ("ì•± ì‚¬ìš©ë²• ì•Œë ¤ì£¼ì„¸ìš”", "ì´ìš©_ë°©ë²•"),
        ("ì°¨ëŸ‰ ë°˜ë‚©ì€ ì–´ë–»ê²Œ í•˜ë‚˜ìš”?", "ì´ìš©_ë°©ë²•"),
        ("ì‹œë™ì€ ì–´ë–»ê²Œ ê±°ë‚˜ìš”?", "ì´ìš©_ë°©ë²•"),
        ("ì£¼ìœ ëŠ” ì–´ë–»ê²Œ í•˜ë‚˜ìš”?", "ì´ìš©_ë°©ë²•"),
        ("ì°¨ í‚¤ëŠ” ì–´ë”” ìˆì–´ìš”?", "ì´ìš©_ë°©ë²•"),
        ("ë°˜ë‚© ì¥ì†Œê°€ ì–´ë””ì¸ê°€ìš”?", "ì´ìš©_ë°©ë²•"),
        ("ì•±ì—ì„œ ì°¨ëŸ‰ ì°¾ê¸° ì–´ë–»ê²Œ í•´ìš”?", "ì´ìš©_ë°©ë²•"),
        
        # ë¬¸ì œ í•´ê²° (10ê°œ)
        ("ì‚¬ê³ ê°€ ë‚¬ì–´ìš” ì–´ë–»ê²Œ í•´ì•¼í•˜ì£ ?", "ë¬¸ì œ_í•´ê²°"),
        ("ì°¨ê°€ ì‹œë™ì´ ì•ˆ ê±¸ë ¤ìš”", "ë¬¸ì œ_í•´ê²°"),
        ("ì•±ì´ ì•ˆ ì—´ë ¤ìš”", "ë¬¸ì œ_í•´ê²°"),
        ("ì°¨ ë¬¸ì´ ì•ˆ ì—´ë ¤ìš”", "ë¬¸ì œ_í•´ê²°"),
        ("ê³ ì¥ ì‹ ê³ í•˜ê³  ì‹¶ì–´ìš”", "ë¬¸ì œ_í•´ê²°"),
        ("ë¬¸ì œê°€ ìƒê²¼ì–´ìš”", "ë¬¸ì œ_í•´ê²°"),
        ("ì°¨ëŸ‰ì— ì´ìƒì´ ìˆì–´ìš”", "ë¬¸ì œ_í•´ê²°"),
        ("ë„ì›€ì´ í•„ìš”í•´ìš”", "ë¬¸ì œ_í•´ê²°"),
        ("ì°¨ê°€ ê³ ì¥ë‚¬ì–´ìš”", "ë¬¸ì œ_í•´ê²°"),
        ("ì‚¬ê³  ì²˜ë¦¬ ì–´ë–»ê²Œ í•˜ë‚˜ìš”?", "ë¬¸ì œ_í•´ê²°"),
        
        # ê³„ì • ê´€ë¦¬ (10ê°œ)
        ("ë¹„ë°€ë²ˆí˜¸ë¥¼ ìŠì–´ë²„ë ¸ì–´ìš”", "ê³„ì •_ê´€ë¦¬"),
        ("íšŒì›ê°€ì…ì€ ì–´ë–»ê²Œ í•˜ë‚˜ìš”?", "ê³„ì •_ê´€ë¦¬"),
        ("ì•„ì´ë”” ì°¾ê¸°", "ê³„ì •_ê´€ë¦¬"),
        ("íšŒì› íƒˆí‡´í•˜ê³  ì‹¶ì–´ìš”", "ê³„ì •_ê´€ë¦¬"),
        ("ë¡œê·¸ì¸ì´ ì•ˆ ë¼ìš”", "ê³„ì •_ê´€ë¦¬"),
        ("ê³„ì • ì •ë³´ ë³€ê²½í•˜ê³  ì‹¶ì–´ìš”", "ê³„ì •_ê´€ë¦¬"),
        ("ë‚´ ì •ë³´ ìˆ˜ì •", "ê³„ì •_ê´€ë¦¬"),
        ("íŒ¨ìŠ¤ì›Œë“œ ì¬ì„¤ì •", "ê³„ì •_ê´€ë¦¬"),
        ("í”„ë¡œí•„ ë³€ê²½", "ê³„ì •_ê´€ë¦¬"),
        ("íšŒì›ì •ë³´ ì—…ë°ì´íŠ¸", "ê³„ì •_ê´€ë¦¬"),
        
        # ì¸ì‚¬ (8ê°œ)
        ("ì•ˆë…•í•˜ì„¸ìš”", "ì¸ì‚¬"),
        ("ì•ˆë…•", "ì¸ì‚¬"),
        ("í•˜ì´", "ì¸ì‚¬"),
        ("ì•ˆë…•í•˜ì‹­ë‹ˆê¹Œ", "ì¸ì‚¬"),
        ("ë°˜ê°‘ìŠµë‹ˆë‹¤", "ì¸ì‚¬"),
        ("ì²˜ìŒ ëµ™ê² ìŠµë‹ˆë‹¤", "ì¸ì‚¬"),
        ("ì¢‹ì€ ì•„ì¹¨ì´ì—ìš”", "ì¸ì‚¬"),
        ("ì•ˆë…•íˆ ê³„ì„¸ìš”", "ì¸ì‚¬"),
        
        # ê°ì‚¬ (8ê°œ)
        ("ê³ ë§ˆì›Œìš”", "ê°ì‚¬"),
        ("ê°ì‚¬í•©ë‹ˆë‹¤", "ê°ì‚¬"),
        ("ë„ì›€ì´ ëì–´ìš”", "ê°ì‚¬"),
        ("ê³ ë§™ìŠµë‹ˆë‹¤", "ê°ì‚¬"),
        ("ê°ì‚¬í•´ìš”", "ê°ì‚¬"),
        ("ì •ë§ ê³ ë§ˆì›Œìš”", "ê°ì‚¬"),
        ("ë§ì€ ë„ì›€ì´ ë˜ì—ˆìŠµë‹ˆë‹¤", "ê°ì‚¬"),
        ("ì¹œì ˆí•˜ê²Œ ì•Œë ¤ì£¼ì…”ì„œ ê°ì‚¬í•©ë‹ˆë‹¤", "ê°ì‚¬"),
    ]
    
    for text, intent in initial_data:
        ml_manager.add_training_data(text, intent)
    
    print(f"[AutoML] ì´ˆê¸° í›ˆë ¨ ë°ì´í„° ì„¤ì • ì™„ë£Œ: {len(initial_data)}ê°œ")
    
    # ì¶©ë¶„í•œ ë°ì´í„°ê°€ ìˆìœ¼ë©´ ì´ˆê¸° í›ˆë ¨
    if len(ml_manager.training_data) >= 20:
        ml_manager.train_intent_classifier()

def session_cleanup_worker():
    """ë°±ê·¸ë¼ìš´ë“œ ì„¸ì…˜ ì •ë¦¬ ì‘ì—… - ìŠ¤ë ˆë“œ ì•ˆì „"""
    while True:
        try:
            cleanup_expired_sessions()
            ml_manager.auto_retrain()
            time.sleep(3600)  # 1ì‹œê°„ë§ˆë‹¤ ì‹¤í–‰
        except Exception as e:
            print(f"[ë°±ê·¸ë¼ìš´ë“œ ì‘ì—… ì˜¤ë¥˜] {e}")
            time.sleep(3600)

# ë°±ê·¸ë¼ìš´ë“œ ìŠ¤ë ˆë“œ ì‹œì‘
cleanup_thread = threading.Thread(target=session_cleanup_worker, daemon=True)
cleanup_thread.start()

# --- API ì—”ë“œí¬ì¸íŠ¸ë“¤ (ê¸°ì¡´ê³¼ ë™ì¼) ---
@app.route("/chat", methods=["POST", "OPTIONS"])
def chat():
    if request.method == "OPTIONS":
        return jsonify({"status": "ok"}), 200
    
    try:
        # ìš”ì²­ íŒŒì‹±
        data = request.get_json(silent=True)
        if isinstance(data, dict):
            message = (data.get("message") or "").strip()
            session_id = data.get("session_id") or str(uuid.uuid4())
        else:
            message = (request.form.get("message") or request.args.get("message") or "").strip()
            session_id = request.form.get("session_id") or request.args.get("session_id") or str(uuid.uuid4())
        
        if not message:
            return jsonify({"error": "ë©”ì‹œì§€ê°€ ì—†ìŠµë‹ˆë‹¤"}), 400
        
        # ì„¸ì…˜ ê°€ì ¸ì˜¤ê¸°
        session = get_or_create_session(session_id)
        
        # ML ì˜ˆì¸¡
        ml_intent, ml_confidence = ml_manager.predict_intent(message)
        
        # ê°„ë‹¨í•œ ì‘ë‹µ ìƒì„± (ì‹¤ì œë¡œëŠ” ë” ë³µì¡í•œ NLG ì‚¬ìš©)
        response = generate_response(message, ml_intent, ml_confidence)
        
        # ë©”ì‹œì§€ ê¸°ë¡
        session.add_message("user", message, {
            "predicted_intent": ml_intent,
            "confidence": ml_confidence
        })
        session.add_message("assistant", response)
        
        return jsonify({
            "response": response,
            "session_id": session_id,
            "ml_prediction": {
                "intent": ml_intent,
                "confidence": ml_confidence
            }
        })
        
    except Exception as e:
        print(f"[Chat API ì˜¤ë¥˜] {e}")
        return jsonify({"error": "ì±„íŒ… ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤"}), 500

def generate_response(message, intent, confidence):
    """ê°„ë‹¨í•œ ì‘ë‹µ ìƒì„±ê¸°"""
    if confidence < 0.3:
        return "ì£„ì†¡í•©ë‹ˆë‹¤. ì •í™•íˆ ì´í•´í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ë§ì”€í•´ ì£¼ì‹œê² ì–´ìš”?"
    
    responses = {
        "ì˜ˆì•½_ë¬¸ì˜": "ì°¨ëŸ‰ ì˜ˆì•½ ê´€ë ¨ ë¬¸ì˜êµ°ìš”! MOCA ì•±ì—ì„œ ì‹¤ì‹œê°„ìœ¼ë¡œ ì˜ˆì•½ ê°€ëŠ¥í•œ ì°¨ëŸ‰ì„ í™•ì¸í•˜ì‹¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤.",
        "ìš”ê¸ˆ_ë¬¸ì˜": "ìš”ê¸ˆ ë¬¸ì˜í•´ ì£¼ì…¨ë„¤ìš”! ê¸°ë³¸ ìš”ê¸ˆì€ ì‹œê°„ë‹¹ 1,000ì›ì´ë©°, ì£¼í–‰ìš”ê¸ˆì€ kmë‹¹ 100ì›ì…ë‹ˆë‹¤.",
        "ì´ìš©_ë°©ë²•": "ì´ìš© ë°©ë²•ì— ëŒ€í•´ ê¶ê¸ˆí•˜ì‹œêµ°ìš”! MOCA ì•±ì„ í†µí•´ ì°¨ëŸ‰ ì˜ˆì•½ë¶€í„° ë°˜ë‚©ê¹Œì§€ ëª¨ë“  ê³¼ì •ì„ ì§„í–‰í•˜ì‹¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤.",
        "ë¬¸ì œ_í•´ê²°": "ë¬¸ì œê°€ ë°œìƒí•˜ì…¨êµ°ìš”! ê³ ê°ì„¼í„°(1588-1234)ë¡œ ì—°ë½ì£¼ì‹œë©´ ì¦‰ì‹œ ë„ì›€ì„ ë“œë¦¬ê² ìŠµë‹ˆë‹¤.",
        "ê³„ì •_ê´€ë¦¬": "ê³„ì • ê´€ë¦¬ ê´€ë ¨ ë¬¸ì˜ì‹œêµ°ìš”! ì•± ì„¤ì •ì—ì„œ ê°œì¸ì •ë³´ë¥¼ ìˆ˜ì •í•˜ì‹œê±°ë‚˜ ê³ ê°ì„¼í„°ë¡œ ë¬¸ì˜í•´ ì£¼ì„¸ìš”.",
        "ì¸ì‚¬": "ì•ˆë…•í•˜ì„¸ìš”! MOCA ì±—ë´‡ì…ë‹ˆë‹¤. ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?",
        "ê°ì‚¬": "ì²œë§Œì—ìš”! ë” ê¶ê¸ˆí•œ ê²ƒì´ ìˆìœ¼ì‹œë©´ ì–¸ì œë“  ë¬¼ì–´ë³´ì„¸ìš”."
    }
    
    return responses.get(intent, "MOCA ì„œë¹„ìŠ¤ì— ëŒ€í•´ ê¶ê¸ˆí•œ ê²ƒì´ ìˆìœ¼ì‹œë©´ ì–¸ì œë“  ë§ì”€í•´ ì£¼ì„¸ìš”!")

@app.route("/ml_stats", methods=["GET", "OPTIONS"])
def get_ml_stats():
    if request.method == "OPTIONS":
        return jsonify({"status": "ok"}), 200
    
    try:
        stats = ml_manager.get_model_stats()
        return jsonify(stats)
    except Exception as e:
        return jsonify({"error": str(e)}), 500

@app.route("/retrain", methods=["POST", "OPTIONS"])
def retrain_model():
    if request.method == "OPTIONS":
        return jsonify({"status": "ok"}), 200
    
    try:
        success = ml_manager.train_intent_classifier()
        if success:
            return jsonify({"status": "retrain completed"})
        else:
            return jsonify({"status": "retrain failed", "error": "insufficient data"}), 400
    except Exception as e:
        return jsonify({"error": str(e)}), 500

@app.route("/sessions", methods=["GET", "OPTIONS"])
def get_all_sessions():
    if request.method == "OPTIONS":
        return jsonify({"status": "ok"}), 200
    
    try:
        sessions_info = []
        with session_lock:
            for sid, session in conversation_sessions.items():
                sessions_info.append({
                    "session_id": sid,
                    "message_count": len(session.messages),
                    "created_at": session.created_at.isoformat(),
                    "last_activity": session.last_activity.isoformat(),
                    "is_expired": session.is_expired()
                })
        
        return jsonify({
            "total_sessions": len(sessions_info),
            "sessions": sessions_info
        })
    except Exception as e:
        return jsonify({"error": str(e)}), 500

# --- ì•± ì‹œì‘ ì‹œ ì´ˆê¸°í™” ---
initialize_training_data()

if __name__ == "__main__":
    print("ğŸš€ MOCA ì±—ë´‡ ì„œë²„ ì‹œì‘ (Thread-Safe AutoML)")
    print(f"ğŸ“Š ì„¸ì…˜ ì„¤ì •: ìµœëŒ€ {MAX_MESSAGES_PER_SESSION}ê°œ ë©”ì‹œì§€, {SESSION_TIMEOUT_HOURS}ì‹œê°„ íƒ€ì„ì•„ì›ƒ")
    print(f"ğŸ¤– AutoML í†µê³„: {ml_manager.get_model_stats()}")
    print("ğŸ”’ ìŠ¤ë ˆë“œ ì•ˆì „ì„±: ëª¨ë“  ML ì‘ì—…ì´ ë½ìœ¼ë¡œ ë³´í˜¸ë¨")
    print("ğŸŒ CORS ì„¤ì •: localhost:3000, 127.0.0.1:3000 í—ˆìš©")
    app.run(host="0.0.0.0", port=5000, debug=False)  # debug=Falseë¡œ ë³€ê²½ (í”„ë¡œë•ì…˜ í™˜ê²½)