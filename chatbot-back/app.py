# app.py - CORS ë¬¸ì œ í•´ê²°ëœ ë²„ì „
from flask import Flask, render_template, request, jsonify
from flask_cors import CORS
from google import genai
from google.genai import types
import os
import traceback
import uuid
from datetime import datetime, timedelta
import threading
import time
import json
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report
import joblib
from collections import Counter, defaultdict

# --- Flask ì•± ìƒì„± ---
app = Flask(__name__)

# --- CORS ì„¤ì • (ìˆ˜ì •ë¨) ---
CORS(app, 
     origins=["http://localhost:3000", "http://127.0.0.1:3000"],  # React ê°œë°œ ì„œë²„
     methods=["GET", "POST", "PUT", "DELETE", "OPTIONS"],
     allow_headers=["Content-Type", "Authorization"],
     supports_credentials=True)

# --- Google Cloud ì¸ì¦ ---
SERVICE_ACCOUNT_KEY_FILE = "application_default_credentials.json"
os.environ["GOOGLE_APPLICATION_CREDENTIALS"] = SERVICE_ACCOUNT_KEY_FILE

# --- GenAI Client ---
genai_client = genai.Client(
    vertexai=True,
    project="moca-chatbot-471203",
    location="asia-northeast1",
)
MODEL_NAME = "gemini-1.5-flash-002"

# --- ë¡œì»¬ ML ëª¨ë¸ ê´€ë¦¬ í´ë˜ìŠ¤ ---
class LocalMLManager:
    def __init__(self):
        self.intent_classifier = None
        self.vectorizer = None
        self.training_data = []
        self.model_path = "models/"
        self.ensure_model_dir()
        self.load_models()
        
        # ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§
        self.prediction_log = []
        self.feedback_data = []
        
    def ensure_model_dir(self):
        """ëª¨ë¸ ë””ë ‰í† ë¦¬ ìƒì„±"""
        os.makedirs(self.model_path, exist_ok=True)
    
    def add_training_data(self, text, intent, confidence=1.0):
        """í›ˆë ¨ ë°ì´í„° ì¶”ê°€"""
        self.training_data.append({
            'text': text,
            'intent': intent,
            'confidence': confidence,
            'timestamp': datetime.now()
        })
    
    def train_intent_classifier(self):
        """ì˜ë„ ë¶„ë¥˜ ëª¨ë¸ í›ˆë ¨"""
        if len(self.training_data) < 10:
            print(f"[AutoML] í›ˆë ¨ ë°ì´í„° ë¶€ì¡±: {len(self.training_data)}ê°œ (ìµœì†Œ 10ê°œ í•„ìš”)")
            return False
        
        # ë°ì´í„° ì¤€ë¹„
        texts = [item['text'] for item in self.training_data]
        intents = [item['intent'] for item in self.training_data]
        
        # í´ë˜ìŠ¤ë³„ ë°ì´í„° ìˆ˜ í™•ì¸
        intent_counts = Counter(intents)
        min_samples_per_class = min(intent_counts.values())
        unique_classes = len(intent_counts)
        
        print(f"[AutoML] í´ë˜ìŠ¤ë³„ ë°ì´í„° ìˆ˜: {dict(intent_counts)}")
        print(f"[AutoML] ì´ ë°ì´í„°: {len(texts)}ê°œ, í´ë˜ìŠ¤: {unique_classes}ê°œ")
        
        # í…ìŠ¤íŠ¸ ë²¡í„°í™”
        self.vectorizer = TfidfVectorizer(
            max_features=1000,
            ngram_range=(1, 2),
            stop_words=None
        )
        X = self.vectorizer.fit_transform(texts)
        
        # ëª¨ë¸ í›ˆë ¨
        self.intent_classifier = RandomForestClassifier(
            n_estimators=100,
            random_state=42,
            class_weight='balanced'
        )
        
        # ë°ì´í„° ë¶„í•  ì¡°ê±´ í™•ì¸ ë° ì¡°ì •
        if len(texts) >= 20 and unique_classes > 1 and min_samples_per_class >= 2:
            try:
                X_train, X_test, y_train, y_test = train_test_split(
                    X, intents, test_size=0.2, random_state=42, stratify=intents
                )
                self.intent_classifier.fit(X_train, y_train)
                
                # ì„±ëŠ¥ í‰ê°€
                y_pred = self.intent_classifier.predict(X_test)
                accuracy = accuracy_score(y_test, y_pred)
                print(f"[AutoML] ì˜ë„ ë¶„ë¥˜ ì •í™•ë„ (stratified): {accuracy:.3f}")
                
            except ValueError as e:
                print(f"[AutoML] Stratify ì‹¤íŒ¨, ì¼ë°˜ ë¶„í• ë¡œ ì§„í–‰: {e}")
                X_train, X_test, y_train, y_test = train_test_split(
                    X, intents, test_size=0.2, random_state=42
                )
                self.intent_classifier.fit(X_train, y_train)
                
                y_pred = self.intent_classifier.predict(X_test)
                accuracy = accuracy_score(y_test, y_pred)
                print(f"[AutoML] ì˜ë„ ë¶„ë¥˜ ì •í™•ë„ (non-stratified): {accuracy:.3f}")
                
        else:
            print(f"[AutoML] ë°ì´í„° ë¶€ì¡±ìœ¼ë¡œ ì „ì²´ ë°ì´í„°ë¡œ í›ˆë ¨ (ê²€ì¦ ì—†ìŒ)")
            self.intent_classifier.fit(X, intents)
        
        # ëª¨ë¸ ì €ì¥
        self.save_models()
        return True
    
    def predict_intent(self, text):
        """ì˜ë„ ì˜ˆì¸¡"""
        if not self.intent_classifier or not self.vectorizer:
            return None, 0.0
        
        try:
            X = self.vectorizer.transform([text])
            intent = self.intent_classifier.predict(X)[0]
            confidence = max(self.intent_classifier.predict_proba(X)[0])
            
            # ì˜ˆì¸¡ ë¡œê·¸ ì €ì¥
            self.prediction_log.append({
                'text': text,
                'predicted_intent': intent,
                'confidence': confidence,
                'timestamp': datetime.now()
            })
            
            return intent, confidence
            
        except Exception as e:
            print(f"[AutoML] ì˜ˆì¸¡ ì˜¤ë¥˜: {e}")
            return None, 0.0
    
    def add_feedback(self, text, predicted_intent, actual_intent, user_satisfied=True):
        """ì‚¬ìš©ì í”¼ë“œë°± ìˆ˜ì§‘"""
        self.feedback_data.append({
            'text': text,
            'predicted_intent': predicted_intent,
            'actual_intent': actual_intent,
            'user_satisfied': user_satisfied,
            'timestamp': datetime.now()
        })
        
        if actual_intent and predicted_intent != actual_intent:
            self.add_training_data(text, actual_intent, confidence=1.0)
    
    def auto_retrain(self):
        """ìë™ ì¬í›ˆë ¨"""
        if len(self.feedback_data) >= 20:
            print("[AutoML] ìë™ ì¬í›ˆë ¨ ì‹œì‘...")
            success = self.train_intent_classifier()
            if success:
                self.feedback_data = []
                print("[AutoML] ìë™ ì¬í›ˆë ¨ ì™„ë£Œ")
    
    def save_models(self):
        """ëª¨ë¸ ì €ì¥"""
        try:
            if self.intent_classifier:
                joblib.dump(self.intent_classifier, f"{self.model_path}/intent_classifier.pkl")
            if self.vectorizer:
                joblib.dump(self.vectorizer, f"{self.model_path}/vectorizer.pkl")
            
            with open(f"{self.model_path}/training_data.json", 'w', encoding='utf-8') as f:
                json.dump(self.training_data, f, ensure_ascii=False, indent=2, default=str)
                
            print("[AutoML] ëª¨ë¸ ì €ì¥ ì™„ë£Œ")
        except Exception as e:
            print(f"[AutoML] ëª¨ë¸ ì €ì¥ ì˜¤ë¥˜: {e}")
    
    def load_models(self):
        """ì €ì¥ëœ ëª¨ë¸ ë¡œë“œ"""
        try:
            classifier_path = f"{self.model_path}/intent_classifier.pkl"
            vectorizer_path = f"{self.model_path}/vectorizer.pkl"
            data_path = f"{self.model_path}/training_data.json"
            
            if os.path.exists(classifier_path):
                self.intent_classifier = joblib.load(classifier_path)
                print("[AutoML] ì˜ë„ ë¶„ë¥˜ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
            
            if os.path.exists(vectorizer_path):
                self.vectorizer = joblib.load(vectorizer_path)
                print("[AutoML] ë²¡í„°ë¼ì´ì € ë¡œë“œ ì™„ë£Œ")
            
            if os.path.exists(data_path):
                with open(data_path, 'r', encoding='utf-8') as f:
                    self.training_data = json.load(f)
                print(f"[AutoML] í›ˆë ¨ ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(self.training_data)}ê°œ")
                
        except Exception as e:
            print(f"[AutoML] ëª¨ë¸ ë¡œë“œ ì˜¤ë¥˜: {e}")
    
    def get_model_stats(self):
        """ëª¨ë¸ í†µê³„ ì •ë³´"""
        return {
            "training_data_count": len(self.training_data),
            "prediction_count": len(self.prediction_log),
            "feedback_count": len(self.feedback_data),
            "model_loaded": self.intent_classifier is not None,
            "vectorizer_loaded": self.vectorizer is not None
        }

# --- ì„¸ì…˜ ê´€ë¦¬ ---
conversation_sessions = {}
session_lock = threading.Lock()
MAX_MESSAGES_PER_SESSION = 20
SESSION_TIMEOUT_HOURS = 2

ml_manager = LocalMLManager()

class ConversationSession:
    def __init__(self, session_id):
        self.session_id = session_id
        self.messages = []
        self.created_at = datetime.now()
        self.last_activity = datetime.now()
        self.ml_predictions = []
    
    def add_message(self, role, content, ml_prediction=None):
        message_data = {
            "role": role,
            "content": content,
            "timestamp": datetime.now(),
            "ml_prediction": ml_prediction
        }
        self.messages.append(message_data)
        self.last_activity = datetime.now()
        
        if len(self.messages) > MAX_MESSAGES_PER_SESSION:
            self.messages = self.messages[-MAX_MESSAGES_PER_SESSION:]
    
    def get_conversation_context(self):
        if not self.messages:
            return ""
        
        context_parts = []
        for msg in self.messages:
            role_label = "ì‚¬ìš©ì" if msg["role"] == "user" else "ì±—ë´‡"
            context_parts.append(f"{role_label}: {msg['content']}")
        
        return "\n".join(context_parts)
    
    def is_expired(self):
        return datetime.now() - self.last_activity > timedelta(hours=SESSION_TIMEOUT_HOURS)

def get_or_create_session(session_id):
    with session_lock:
        if session_id not in conversation_sessions:
            conversation_sessions[session_id] = ConversationSession(session_id)
        return conversation_sessions[session_id]

def cleanup_expired_sessions():
    with session_lock:
        expired_sessions = [
            sid for sid, session in conversation_sessions.items() 
            if session.is_expired()
        ]
        for sid in expired_sessions:
            del conversation_sessions[sid]
        
        if expired_sessions:
            print(f"[ì„¸ì…˜ ì •ë¦¬] {len(expired_sessions)}ê°œ ë§Œë£Œëœ ì„¸ì…˜ ì‚­ì œ")

def initialize_training_data():
    """ì´ˆê¸° í›ˆë ¨ ë°ì´í„° ì„¤ì •"""
    initial_data = [
        # ì˜ˆì•½ ë¬¸ì˜ (10ê°œ)
        ("ì§€ê¸ˆ ë°”ë¡œ íƒˆ ìˆ˜ ìˆëŠ” ì°¨ ìˆì–´ìš”?", "ì˜ˆì•½_ë¬¸ì˜"),
        ("ë‚´ì¼ ì•„ì¹¨ì— ì°¨ ì˜ˆì•½í•˜ê³  ì‹¶ì–´ìš”", "ì˜ˆì•½_ë¬¸ì˜"),
        ("ì°¨ëŸ‰ ì˜ˆì•½ ê°€ëŠ¥í•œê°€ìš”?", "ì˜ˆì•½_ë¬¸ì˜"),
        ("ì˜ˆì•½ ì·¨ì†Œí•˜ê³  ì‹¶ì–´ìš”", "ì˜ˆì•½_ë¬¸ì˜"),
        ("ì˜ˆì•½ ë³€ê²½ ê°€ëŠ¥í•œê°€ìš”?", "ì˜ˆì•½_ë¬¸ì˜"),
        ("ì˜¤ëŠ˜ ì €ë…ì— ì°¨ ë¹Œë¦´ ìˆ˜ ìˆë‚˜ìš”?", "ì˜ˆì•½_ë¬¸ì˜"),
        ("ì˜ˆì•½í•˜ë ¤ë©´ ì–´ë–»ê²Œ í•´ì•¼ í•˜ë‚˜ìš”?", "ì˜ˆì•½_ë¬¸ì˜"),
        ("ì°¨ ì˜ˆì•½ ì‹ ì²­í•˜ê³  ì‹¶ìŠµë‹ˆë‹¤", "ì˜ˆì•½_ë¬¸ì˜"),
        ("ë‹¹ì¼ ì˜ˆì•½ ê°€ëŠ¥í•œê°€ìš”?", "ì˜ˆì•½_ë¬¸ì˜"),
        ("ì˜ˆì•½ í™•ì¸í•˜ê³  ì‹¶ì–´ìš”", "ì˜ˆì•½_ë¬¸ì˜"),
        
        # ìš”ê¸ˆ ë¬¸ì˜ (10ê°œ)
        ("ì£¼í–‰ ìš”ê¸ˆì€ 1kmë‹¹ ì–¼ë§ˆì˜ˆìš”?", "ìš”ê¸ˆ_ë¬¸ì˜"),
        ("í•˜ë£¨ ë ŒíŠ¸ë¹„ê°€ ì–¼ë§ˆì¸ê°€ìš”?", "ìš”ê¸ˆ_ë¬¸ì˜"),
        ("ë³´í—˜ë£ŒëŠ” ë³„ë„ì¸ê°€ìš”?", "ìš”ê¸ˆ_ë¬¸ì˜"),
        ("í• ì¸ í˜œíƒ ìˆë‚˜ìš”?", "ìš”ê¸ˆ_ë¬¸ì˜"),
        ("ê²°ì œ ë°©ë²•ì´ ê¶ê¸ˆí•´ìš”", "ìš”ê¸ˆ_ë¬¸ì˜"),
        ("ìš”ê¸ˆí‘œ ë³´ì—¬ì£¼ì„¸ìš”", "ìš”ê¸ˆ_ë¬¸ì˜"),
        ("ì–¼ë§ˆë‚˜ ë¹„ì‹¸ìš”?", "ìš”ê¸ˆ_ë¬¸ì˜"),
        ("ê°€ê²©ì´ ê¶ê¸ˆí•©ë‹ˆë‹¤", "ìš”ê¸ˆ_ë¬¸ì˜"),
        ("ìš”ê¸ˆ ê³„ì‚° ë°©ë²•ì„ ì•Œë ¤ì£¼ì„¸ìš”", "ìš”ê¸ˆ_ë¬¸ì˜"),
        ("ë¹„ìš©ì´ ì–´ë–»ê²Œ ë˜ë‚˜ìš”?", "ìš”ê¸ˆ_ë¬¸ì˜"),
        
        # ì´ìš© ë°©ë²• (10ê°œ)
        ("ì°¨ ë¬¸ì€ ì–´ë–»ê²Œ ì—´ì–´ìš”?", "ì´ìš©_ë°©ë²•"),
        ("ì•± ì‚¬ìš©ë²• ì•Œë ¤ì£¼ì„¸ìš”", "ì´ìš©_ë°©ë²•"),
        ("ì°¨ëŸ‰ ë°˜ë‚©ì€ ì–´ë–»ê²Œ í•˜ë‚˜ìš”?", "ì´ìš©_ë°©ë²•"),
        ("ì‹œë™ì€ ì–´ë–»ê²Œ ê±°ë‚˜ìš”?", "ì´ìš©_ë°©ë²•"),
        ("ì£¼ìœ ëŠ” ì–´ë–»ê²Œ í•˜ë‚˜ìš”?", "ì´ìš©_ë°©ë²•"),
        ("ì‚¬ìš©ë²•ì´ ê¶ê¸ˆí•´ìš”", "ì´ìš©_ë°©ë²•"),
        ("ì–´ë–»ê²Œ ì´ìš©í•˜ë‚˜ìš”?", "ì´ìš©_ë°©ë²•"),
        ("ì°¨ í‚¤ëŠ” ì–´ë”” ìˆì–´ìš”?", "ì´ìš©_ë°©ë²•"),
        ("ë°˜ë‚© ì¥ì†Œê°€ ì–´ë””ì¸ê°€ìš”?", "ì´ìš©_ë°©ë²•"),
        ("ì•±ì—ì„œ ì°¨ëŸ‰ ì°¾ê¸° ì–´ë–»ê²Œ í•´ìš”?", "ì´ìš©_ë°©ë²•"),
        
        # ë¬¸ì œ í•´ê²° (10ê°œ)
        ("ì‚¬ê³ ê°€ ë‚¬ì–´ìš” ì–´ë–»ê²Œ í•´ì•¼í•˜ì£ ?", "ë¬¸ì œ_í•´ê²°"),
        ("ì°¨ê°€ ì‹œë™ì´ ì•ˆ ê±¸ë ¤ìš”", "ë¬¸ì œ_í•´ê²°"),
        ("ì•±ì´ ì•ˆ ì—´ë ¤ìš”", "ë¬¸ì œ_í•´ê²°"),
        ("ì°¨ ë¬¸ì´ ì•ˆ ì—´ë ¤ìš”", "ë¬¸ì œ_í•´ê²°"),
        ("ê³ ì¥ ì‹ ê³ í•˜ê³  ì‹¶ì–´ìš”", "ë¬¸ì œ_í•´ê²°"),
        ("ë¬¸ì œê°€ ìƒê²¼ì–´ìš”", "ë¬¸ì œ_í•´ê²°"),
        ("ì°¨ëŸ‰ì— ì´ìƒì´ ìˆì–´ìš”", "ë¬¸ì œ_í•´ê²°"),
        ("ë„ì›€ì´ í•„ìš”í•´ìš”", "ë¬¸ì œ_í•´ê²°"),
        ("ì°¨ê°€ ê³ ì¥ë‚¬ì–´ìš”", "ë¬¸ì œ_í•´ê²°"),
        ("ì‚¬ê³  ì²˜ë¦¬ ì–´ë–»ê²Œ í•˜ë‚˜ìš”?", "ë¬¸ì œ_í•´ê²°"),
        
        # ê³„ì • ê´€ë¦¬ (10ê°œ)
        ("ë¹„ë°€ë²ˆí˜¸ë¥¼ ìŠì–´ë²„ë ¸ì–´ìš”", "ê³„ì •_ê´€ë¦¬"),
        ("íšŒì›ê°€ì…ì€ ì–´ë–»ê²Œ í•˜ë‚˜ìš”?", "ê³„ì •_ê´€ë¦¬"),
        ("ì•„ì´ë”” ì°¾ê¸°", "ê³„ì •_ê´€ë¦¬"),
        ("íšŒì› íƒˆí‡´í•˜ê³  ì‹¶ì–´ìš”", "ê³„ì •_ê´€ë¦¬"),
        ("ë¡œê·¸ì¸ì´ ì•ˆ ë¼ìš”", "ê³„ì •_ê´€ë¦¬"),
        ("ê³„ì • ì •ë³´ ë³€ê²½í•˜ê³  ì‹¶ì–´ìš”", "ê³„ì •_ê´€ë¦¬"),
        ("ë‚´ ì •ë³´ ìˆ˜ì •", "ê³„ì •_ê´€ë¦¬"),
        ("íŒ¨ìŠ¤ì›Œë“œ ì¬ì„¤ì •", "ê³„ì •_ê´€ë¦¬"),
        ("í”„ë¡œí•„ ë³€ê²½", "ê³„ì •_ê´€ë¦¬"),
        ("íšŒì›ì •ë³´ ì—…ë°ì´íŠ¸", "ê³„ì •_ê´€ë¦¬"),
        
        # ì¸ì‚¬ (8ê°œ)
        ("ì•ˆë…•í•˜ì„¸ìš”", "ì¸ì‚¬"),
        ("ì•ˆë…•", "ì¸ì‚¬"),
        ("í•˜ì´", "ì¸ì‚¬"),
        ("ì•ˆë…•í•˜ì‹­ë‹ˆê¹Œ", "ì¸ì‚¬"),
        ("ë°˜ê°‘ìŠµë‹ˆë‹¤", "ì¸ì‚¬"),
        ("ì²˜ìŒ ëµ™ê² ìŠµë‹ˆë‹¤", "ì¸ì‚¬"),
        ("ì¢‹ì€ ì•„ì¹¨ì´ì—ìš”", "ì¸ì‚¬"),
        ("ì•ˆë…•íˆ ê³„ì„¸ìš”", "ì¸ì‚¬"),
        
        # ê°ì‚¬ (8ê°œ)
        ("ê³ ë§ˆì›Œìš”", "ê°ì‚¬"),
        ("ê°ì‚¬í•©ë‹ˆë‹¤", "ê°ì‚¬"),
        ("ë„ì›€ì´ ëì–´ìš”", "ê°ì‚¬"),
        ("ê³ ë§™ìŠµë‹ˆë‹¤", "ê°ì‚¬"),
        ("ê°ì‚¬í•´ìš”", "ê°ì‚¬"),
        ("ì •ë§ ê³ ë§ˆì›Œìš”", "ê°ì‚¬"),
        ("ë§ì€ ë„ì›€ì´ ë˜ì—ˆìŠµë‹ˆë‹¤", "ê°ì‚¬"),
        ("ì¹œì ˆí•˜ê²Œ ì•Œë ¤ì£¼ì…”ì„œ ê°ì‚¬í•©ë‹ˆë‹¤", "ê°ì‚¬"),
    ]
    
    for text, intent in initial_data:
        ml_manager.add_training_data(text, intent)
    
    print(f"[AutoML] ì´ˆê¸° í›ˆë ¨ ë°ì´í„° ì„¤ì • ì™„ë£Œ: {len(initial_data)}ê°œ")
    
    if len(ml_manager.training_data) >= 20:
        ml_manager.train_intent_classifier()

def session_cleanup_worker():
    while True:
        try:
            cleanup_expired_sessions()
            ml_manager.auto_retrain()
            time.sleep(3600)
        except Exception as e:
            print(f"[ë°±ê·¸ë¼ìš´ë“œ ì‘ì—… ì˜¤ë¥˜] {e}")
            time.sleep(3600)

cleanup_thread = threading.Thread(target=session_cleanup_worker, daemon=True)
cleanup_thread.start()

def extract_message_and_session(req) -> tuple:
    data = req.get_json(silent=True)
    
    if isinstance(data, dict):
        message = (data.get("message") or "").strip()
        session_id = data.get("session_id") or str(uuid.uuid4())
        return message, session_id
    
    message = (req.form.get("message") or req.args.get("message") or "").strip()
    session_id = req.form.get("session_id") or req.args.get("session_id") or str(uuid.uuid4())
    
    return message, session_id

def call_model_with_automl(current_message: str, conversation_context: str = "") -> tuple:
    ml_intent, ml_confidence = ml_manager.predict_intent(current_message)
    
    classification_prompt = f"""ì‚¬ìš©ìì˜ ì§ˆë¬¸ ì˜ë„ë¥¼ MOCA(ëª¨ì¹´) ì•±ì˜ ê¸°ëŠ¥ì— ë”°ë¼ ì•„ë˜ [ì¹´í…Œê³ ë¦¬] ì¤‘ í•˜ë‚˜ë¡œë§Œ ë¶„ë¥˜í•˜ì„¸ìš”.

[ì¹´í…Œê³ ë¦¬]
ì˜ˆì•½_ë¬¸ì˜, ìš”ê¸ˆ_ë¬¸ì˜, ì´ìš©_ë°©ë²•, ë¬¸ì œ_í•´ê²°, ê³„ì •_ê´€ë¦¬, ì¸ì‚¬, ê°ì‚¬, ê¸°íƒ€_ë¬¸ì˜

ì§ˆë¬¸: {current_message}
ì •ë‹µ(ì¹´í…Œê³ ë¦¬ë§Œ):"""

    contents = [types.Content(role="user", parts=[types.Part.from_text(text=classification_prompt)])]
    cfg = types.GenerateContentConfig(temperature=0.3, max_output_tokens=50)
    
    gemini_intent = "ê¸°íƒ€_ë¬¸ì˜"
    try:
        resp = genai_client.models.generate_content(
            model=MODEL_NAME, contents=contents, config=cfg
        )
        gemini_intent = (getattr(resp, "text", None) or "ê¸°íƒ€_ë¬¸ì˜").split()[0].strip()
    except Exception as e:
        print(f"[Gemini ì˜ë„ ë¶„ë¥˜ ì˜¤ë¥˜] {e}")

    final_intent = gemini_intent
    prediction_source = "gemini"
    
    if ml_intent and ml_confidence > 0.7:
        final_intent = ml_intent
        prediction_source = "local_ml"
    elif ml_intent and ml_confidence > 0.5:
        if ml_intent == gemini_intent:
            final_intent = ml_intent
            prediction_source = "consensus"
    
    print(f"[AutoML] ML: {ml_intent}({ml_confidence:.2f}), Gemini: {gemini_intent}, Final: {final_intent} ({prediction_source})")

    generation_guidelines = {
        "ì˜ˆì•½_ë¬¸ì˜": "ì‚¬ìš©ìê°€ ì˜ˆì•½ì„ ì›í•©ë‹ˆë‹¤. ì›í•˜ëŠ” ì°¨ì¢…/ì§€ì—­/ë‚ ì§œ/ì‹œê°„ì„ ì •ì¤‘íˆ ë¬¼ì–´ë³´ì„¸ìš”.",
        "ìš”ê¸ˆ_ë¬¸ì˜": "ìš”ê¸ˆì´ ê¶ê¸ˆí•©ë‹ˆë‹¤. êµ¬ì²´ì ìœ¼ë¡œ ì°¨ëŸ‰/ê¸°ê°„/ë³´í—˜ ì—¬ë¶€ë¥¼ ë¬¼ì–´ë³´ë©° ê¸°ë³¸ ì•ˆë‚´ë¥¼ ì‹œì‘í•˜ì„¸ìš”.",
        "ì´ìš©_ë°©ë²•": "ì•±/ì°¨ëŸ‰ ì´ìš© ë°©ë²•ì„ ë¬»ê³  ìˆìŠµë‹ˆë‹¤. í•„ìš”í•œ ê¸°ëŠ¥ì´ë‚˜ í™”ë©´ì„ ë¬¼ì–´ë³´ì„¸ìš”.",
        "ë¬¸ì œ_í•´ê²°": "ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ê³µê° í‘œí˜„ í›„ ì–´ë–¤ í™”ë©´/ì˜¤ë¥˜ë©”ì‹œì§€ì¸ì§€ ë¬¼ì–´ë³´ê³  ì¡°ì¹˜ ë‹¨ê³„ë¥¼ ì•ˆë‚´í•˜ì„¸ìš”.",
        "ê³„ì •_ê´€ë¦¬": "ê³„ì • ê´€ë ¨ ë¬¸ì˜ì…ë‹ˆë‹¤. ì•„ì´ë””/ë¹„ë°€ë²ˆí˜¸/ë¡œê·¸ì¸ ë¬¸ì œ ë“± êµ¬ì²´ í•­ëª©ì„ ë¬¼ì–´ë³´ì„¸ìš”.",
        "ì¸ì‚¬": "ë°ê³  ê°„ë‹¨íˆ ì¸ì‚¬í•˜ê³ , ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ì§€ ë¬¼ì–´ë³´ì„¸ìš”.",
        "ê°ì‚¬": "ì •ì¤‘íˆ ë‹µí•˜ê³ , ì¶”ê°€ë¡œ ë„ì™€ë“œë¦´ ì¼ì´ ìˆëŠ”ì§€ ë¬¼ì–´ë³´ì„¸ìš”.",
        "ê¸°íƒ€_ë¬¸ì˜": "ì˜ë„ê°€ ë¶ˆëª…í™•í•©ë‹ˆë‹¤. ì–´ë–¤ ë„ì›€ì„ ì›í•˜ì‹œëŠ”ì§€ í•œ ë¬¸ì¥ìœ¼ë¡œ êµ¬ì²´ì ìœ¼ë¡œ ë¬¼ì–´ë³´ì„¸ìš”.",
    }
    guideline = generation_guidelines.get(final_intent, generation_guidelines["ê¸°íƒ€_ë¬¸ì˜"])

    context_instruction = ""
    if conversation_context:
        context_instruction = f"""
[ì´ì „ ëŒ€í™” ê¸°ë¡]
{conversation_context}

ìœ„ ëŒ€í™” ë§¥ë½ì„ ì°¸ê³ í•˜ì—¬ ë‹µë³€í•˜ì„¸ìš”."""

    generation_prompt = f"""ë‹¹ì‹ ì€ MOCA ì°¨ëŸ‰ ë Œíƒˆ ì„œë¹„ìŠ¤ì˜ ê³ ê°ì§€ì› ì±—ë´‡ì…ë‹ˆë‹¤.
ì•„ë˜ [ê°€ì´ë“œë¼ì¸]ì„ ë”°ë¥´ë˜ ê³¼ì¥ ì—†ì´ ì •ì¤‘í•˜ê³  ê°„ê²°í•˜ê²Œ í•œêµ­ì–´ë¡œ ë‹µí•˜ì„¸ìš”.
{context_instruction}

[ê°€ì´ë“œë¼ì¸]: {guideline}
[í˜„ì¬ ì‚¬ìš©ì ì§ˆë¬¸]: {current_message}

[ì¶œë ¥]:"""

    contents = [types.Content(role="user", parts=[types.Part.from_text(text=generation_prompt)])]
    cfg = types.GenerateContentConfig(temperature=0.7, top_p=0.9, max_output_tokens=1024)
    
    response_text = "ì£„ì†¡í•©ë‹ˆë‹¤. ë‹¤ì‹œ í•œ ë²ˆ ë§ì”€í•´ ì£¼ì‹œê² ì–´ìš”?"
    try:
        resp = genai_client.models.generate_content(
            model=MODEL_NAME, contents=contents, config=cfg
        )
        response_text = (getattr(resp, "text", None) or "").strip()
        if not response_text:
            response_text = "ì£„ì†¡í•©ë‹ˆë‹¤. ë‹¤ì‹œ í•œ ë²ˆ ë§ì”€í•´ ì£¼ì‹œê² ì–´ìš”?"
    except Exception as e:
        print(f"[ë‹µë³€ ìƒì„± ì˜¤ë¥˜] {e}")
        response_text = "ì„œë²„ì— ì¼ì‹œì ì¸ ë¬¸ì œê°€ ìˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ ì£¼ì„¸ìš”."
    
    ml_prediction = {
        "ml_intent": ml_intent,
        "ml_confidence": ml_confidence,
        "gemini_intent": gemini_intent,
        "final_intent": final_intent,
        "prediction_source": prediction_source
    }
    
    return response_text, ml_prediction

# --- Routes ---
@app.route("/")
def home():
    try:
        return render_template("index.html")
    except Exception:
        return "MOCA Chatbot Backend with AutoML is running."

@app.route("/test")
def test():
    stats = ml_manager.get_model_stats()
    return jsonify({
        "status": "ok", 
        "sessions": len(conversation_sessions),
        "automl_stats": stats
    })

@app.route("/get_response", methods=["POST", "OPTIONS"])
def get_response():
    # OPTIONS ìš”ì²­ ì²˜ë¦¬ (CORS preflight)
    if request.method == "OPTIONS":
        return jsonify({"status": "ok"}), 200
        
    try:
        user_message, session_id = extract_message_and_session(request)
        
        if not user_message:
            return jsonify({"error": "message is required"}), 400

        session = get_or_create_session(session_id)
        conversation_context = session.get_conversation_context()
        
        response_text, ml_prediction = call_model_with_automl(user_message, conversation_context)
        
        session.add_message("user", user_message)
        session.add_message("assistant", response_text, ml_prediction)
        
        return jsonify({
            "response": response_text,
            "session_id": session_id,
            "ml_prediction": ml_prediction
        }), 200

    except Exception as e:
        print("[/get_response] INTERNAL ERROR:", e)
        traceback.print_exc()
        return jsonify({"error": "internal server error"}), 500

@app.route("/feedback", methods=["POST", "OPTIONS"])
def submit_feedback():
    if request.method == "OPTIONS":
        return jsonify({"status": "ok"}), 200
        
    try:
        data = request.get_json()
        text = data.get("text")
        predicted_intent = data.get("predicted_intent")
        actual_intent = data.get("actual_intent")
        user_satisfied = data.get("user_satisfied", True)
        
        if text and predicted_intent:
            ml_manager.add_feedback(text, predicted_intent, actual_intent, user_satisfied)
            return jsonify({"status": "feedback recorded"}), 200
        else:
            return jsonify({"error": "invalid feedback data"}), 400
            
    except Exception as e:
        print(f"[í”¼ë“œë°± ì˜¤ë¥˜] {e}")
        return jsonify({"error": "feedback processing failed"}), 500

@app.route("/retrain", methods=["POST", "OPTIONS"])
def manual_retrain():
    if request.method == "OPTIONS":
        return jsonify({"status": "ok"}), 200
        
    try:
        success = ml_manager.train_intent_classifier()
        if success:
            return jsonify({"status": "retrain completed", "stats": ml_manager.get_model_stats()}), 200
        else:
            return jsonify({"error": "insufficient training data"}), 400
    except Exception as e:
        print(f"[ì¬í›ˆë ¨ ì˜¤ë¥˜] {e}")
        return jsonify({"error": "retrain failed"}), 500

@app.route("/ml_stats", methods=["GET", "OPTIONS"])
def get_ml_stats():
    if request.method == "OPTIONS":
        return jsonify({"status": "ok"}), 200
        
    try:
        stats = ml_manager.get_model_stats()
        
        recent_predictions = ml_manager.prediction_log[-100:]
        recent_accuracy = 0.0
        if recent_predictions and ml_manager.feedback_data:
            correct_predictions = sum(1 for fb in ml_manager.feedback_data[-50:] 
                                    if fb['predicted_intent'] == fb['actual_intent'])
            recent_accuracy = correct_predictions / min(len(ml_manager.feedback_data), 50) if ml_manager.feedback_data else 0
        
        stats["recent_accuracy"] = recent_accuracy
        return jsonify(stats), 200
        
    except Exception as e:
        return jsonify({"error": str(e)}), 500

@app.route("/session/<session_id>/history", methods=["GET", "OPTIONS"])
def get_session_history(session_id):
    if request.method == "OPTIONS":
        return jsonify({"status": "ok"}), 200
        
    try:
        if session_id in conversation_sessions:
            session = conversation_sessions[session_id]
            return jsonify({
                "session_id": session_id,
                "message_count": len(session.messages),
                "created_at": session.created_at.isoformat(),
                "last_activity": session.last_activity.isoformat(),
                "messages": session.messages
            })
        else:
            return jsonify({"error": "session not found"}), 404
    except Exception as e:
        return jsonify({"error": str(e)}), 500

@app.route("/sessions", methods=["GET", "OPTIONS"])
def get_all_sessions():
    if request.method == "OPTIONS":
        return jsonify({"status": "ok"}), 200
        
    try:
        sessions_info = []
        with session_lock:
            for sid, session in conversation_sessions.items():
                sessions_info.append({
                    "session_id": sid,
                    "message_count": len(session.messages),
                    "created_at": session.created_at.isoformat(),
                    "last_activity": session.last_activity.isoformat(),
                    "is_expired": session.is_expired()
                })
        
        return jsonify({
            "total_sessions": len(sessions_info),
            "sessions": sessions_info
        })
    except Exception as e:
        return jsonify({"error": str(e)}), 500

# --- ì•± ì‹œì‘ ì‹œ ì´ˆê¸°í™” ---
initialize_training_data()

if __name__ == "__main__":
    print("ğŸš€ MOCA ì±—ë´‡ ì„œë²„ ì‹œì‘ (AutoML í†µí•© + CORS í•´ê²°)")
    print(f"ğŸ“Š ì„¸ì…˜ ì„¤ì •: ìµœëŒ€ {MAX_MESSAGES_PER_SESSION}ê°œ ë©”ì‹œì§€, {SESSION_TIMEOUT_HOURS}ì‹œê°„ íƒ€ì„ì•„ì›ƒ")
    print(f"ğŸ¤– AutoML í†µê³„: {ml_manager.get_model_stats()}")
    print("ğŸŒ CORS ì„¤ì •: localhost:3000, 127.0.0.1:3000 í—ˆìš©")
    app.run(host="0.0.0.0", port=5000, debug=True)